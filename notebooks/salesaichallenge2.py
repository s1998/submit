# -*- coding: utf-8 -*-
"""salesAiChallenge2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1met_mDYgxAoDXYm3ivAqFsh3Fm1YTH66
"""

# !pip install transformers datasets

import pandas as pd
df_ques = pd.read_csv('questions_labelled.csv')
df_stat = pd.read_csv('statements_labelled.csv')

# df_ques
# (df_ques['label'] == 2) | (df_ques.index < 250)
# df_ques[((df_ques['label'] == 2) | (df_ques.index < 250)) & (df_ques['label'] != 0) ]
# df_ques[(df_ques['label'] == 0)]
# df_stat.sample(n=250, random_state=1)['sent'].tolist() + df_ques[(df_ques['label'] == 0)]['sent'].tolist()

statements = df_stat.sample(n=250, random_state=1)['sent'].tolist() + df_ques[(df_ques['label'] == 0)]['sent'].tolist()
questions  = df_ques[((df_ques['label'] == 2) | (df_ques.index < 250)) & (df_ques['label'] != 0) ]['sent'].tolist()
print("\n".join(statements[:5] + statements[-5:]), "\n****\n", "\n".join(questions[:5] + questions[-5:]))

from transformers import AutoTokenizer
modelname = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(modelname)

def process_data(text, labelgiven=0):
    text = ' '.join(text.split())
    encodings = tokenizer(text, padding="max_length", truncation=True, max_length=128)
    encodings['label'] = labelgiven
    encodings['text'] = text
    return encodings

process_data("Hi I'm here", 0)

processed_data = []

for stat in statements:
  processed_data.append(process_data(stat, 0))

for ques in questions:
  processed_data.append(process_data(ques, 1))

from sklearn.model_selection import train_test_split
new_df = pd.DataFrame(processed_data)
train_df, valid_df = train_test_split(
    new_df,
    test_size=0.2,
    random_state=2022
)

import pyarrow as pa
from datasets import Dataset

train_hg = Dataset(pa.Table.from_pandas(train_df))
valid_hg = Dataset(pa.Table.from_pandas(valid_df))

from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    modelname,
    num_labels=2
)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="./result", evaluation_strategy="epoch", num_train_epochs=5, warmup_ratio=0.1, learning_rate=0.00001)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_hg,
    eval_dataset=valid_hg,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
print(trainer.evaluate())
model.save_pretrained('./model/')

print(trainer.evaluate())

# ## Load the model

# In[14]:


from transformers import AutoModelForSequenceClassification

new_model = AutoModelForSequenceClassification.from_pretrained('./model/').to(trainer.model.device)


# In[15]:


from transformers import AutoTokenizer

new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')


# ## Get predictions

# In[16]:


import torch
import numpy as np

def get_prediction(text):
    encoding = new_tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

    outputs = new_model(**encoding)

    logits = outputs.logits

    sigmoid = torch.nn.Softmax(dim=-1)
    probs = sigmoid(logits.squeeze().cpu())
    probs = probs.detach().numpy()
    label = np.argmax(probs, axis=-1)
    
    if label == 1:
        return ('question', probs[1])
    else:
        return ('statement', probs[0])


# In[17]:

sents = [
    'what did you have for lunch?',
    'this is urgent, when can you deliver',
    'can you do this in a month',
    'this is good work',
    'can you pull up the numbers for me',
    'why is the delay',
    'how is the baby doing',
    'what are the plans for the weekend',
    'what are the plans for the birthday',
    'I can hear you saying that'
]

for sent in sents:
  print(sent, get_prediction(sent))

# trainer.model.device

sents = [
    'what did you have for lunch?',
    'this is urgent, when can you deliver',
    'can you do this in a month',
    'this is good work',
    'can you pull up the numbers for me',
    'why is the delay',
    'how is the baby doing',
    'what are the plans for the weekend',
    'what are the plans for the birthday',
    'how was the weekend',
    'did you have fun',
    'Am I audible?',
    'Can you hear me?',
    'I can hear you saying that',
    'Can you explain me the subscription model again?',
    'What are the accepted payment menthods?',
    'When are you leaving for the vacation?',
    'I wanna help on real Quick can just I need like, two more minutes is that okay?',
    'I I in hear, for example?',
    'Can you guys hear me?',
    'And are you interested that be ready by January one are you guys just sort that?',
    'What you\'re calling from Hot hubspot?',
    'Can you give me a minute?',
    'High How are you?',
    ' Did you you create a seat for rachel lancaster?',
    'But Can you still hear me?',
    'We don\'t need to have hundreds of an hour shown, you know?',
    'What are you doing in holidays?',
    'Is the software supposed to work like this?',
    'Is your product better than gong?',
    'What would be a good place to learn the usage of software?'
]

for sent in sents:
  print(sent, get_prediction(sent))



# from google.colab import drive
# drive.mount('/content/gdrive')

# !cp /content/model/* /content/gdrive/My\ Drive

