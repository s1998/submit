# -*- coding: utf-8 -*-
"""newSalesAiChallenge3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1bBcddx-SIGApwu2qZsplS0R9mkIZ07
"""

# !pip install transformers flair

# !unzip transcripts-20230117T054959Z-001.zip

# !ls transcripts

!pip install allennlp==2.1.0 allennlp-models==2.1.0
!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load('en_core_web_sm') # Load the English Model

import os, re

all_sents = []

for i, filename in enumerate(sorted(list(os.listdir("transcripts")))[:20]):
  print(i, filename)
  fname = "transcripts/" + filename
  with open(fname, "r") as f:
    txt = f.readlines()
  for l in txt:
    l = l.strip()
    l = " ".join(l.split(":")[1:])
    doc = nlp(l)
    for sent in doc.sents:
      all_sents.append(str(sent))
    # print(l)
    # print(all_sents[-5:])
  # break

# all_sents

from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz",
                                cuda_device=0)
kk = predictor.predict(
  sentence="Did Uriah honestly think he could beat the game in under three hours?."
)

print(kk)

kk = predictor.predict(
  sentence="Did Uriah honestly think he could beat the game in under three hours?."
)

print(kk)
for s in kk['verbs']:
  print(s)

for i in range(150, 160):
  sent = all_sents[i]
  print(all_sents[i-5:i])
  print(sent)
  kk = predictor.predict(
    sentence=sent)
  for s in kk['verbs']:
    print("    ", s)
  print("\n\n")

processed_sent_context_tuples = []

for i in range(100, 110):
  sent = all_sents[i]
  context_sent = all_sents[i-5:i]
  print(context_sent)
  print(sent)
  kk = predictor.predict(sentence=sent)
  for s in kk['verbs']:
    print("    ", s)
  print("\n\n")

for kk in (predictor.predict(sentence="Do you think we could potentially move the demo to sometime next week, Possibly we could do next Monday."))['verbs']:
  print(kk)

sent = "So is he confirmed for tomorrow, just he hasn't accepted the invite yet?"

for kk in (predictor.predict(sentence=sent))['verbs']:
  print(kk)





len(all_sents)

from tqdm.notebook import tqdm

included_sents = []
for sent in tqdm(all_sents):
  kk = predictor.predict(sentence=sent)
  verbs = kk['verbs']

  times = []
  vbs_lst = []
  for verb in verbs:
    include = False
    for t in verb['tags']:
      if 'tmp' in t.lower():
        include = True

    if include:
      times.append([x for x, y in zip(kk['words'], verb['tags']) if 'tmp' in y.lower()])
      vbs_lst.append(verb["verb"])

  if len(times):
    included_sents.append((times, vbs_lst, sent))

for sent in included_sents[:100]:
  print(sent)



# included_sents[0]



import csv
with open('srlTimeV2.csv','w') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['label', 'time', 'verbs', 'sentence'])
    for row in included_sents:
        csv_out.writerow((0, row[0], row[1], row[2]))

# print("hello world")



# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# import torch

# model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-roberta-base')
# tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-roberta-base')

# features = tokenizer(['But his email then you\'re were like, let\'s do it next week.', 'The action "do" is scheduled for future.'], 
#                      ['Hey I\'m just curious, are you guys on Zoom right now?', 'The action is scheduled for future.'],  
#                      padding=True, truncation=True, return_tensors="pt")

# model.eval()
# with torch.no_grad():
#     scores = model(**features).logits
#     label_mapping = ['contradiction', 'entailment', 'neutral']
#     labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]
#     print(labels)

# features = tokenizer(['But his email then you\'re were like, let\'s do it next week.', 'Previous sentence contains a future task.'], 
#                      ['Hey I\'m just curious, are you guys on Zoom right now?', 'Previous sentence contains a future task.'],  
#                      ['Hey I\'m just curious, are you guys on Zoom right now?', 'Previous sentence contains a future task.'],  
#                      ['Hey I\'m just curious, are you guys on Zoom right now?', 'Previous sentence contains a future task.'],  
#                      padding=True, truncation=True, return_tensors="pt")

# sftm = torch.nn.Softmax(dim=1)
# model.eval()
# with torch.no_grad():
#     scores = model(**features).logits
#     scores = sftm(scores)
#     label_mapping = ['contradiction', 'entailment', 'neutral']
#     labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]
#     labels_score = [score_max for score_max in scores.max(dim=1)]
#     print(labels, labels_score)

# scores.max(dim=1), scores



# new_sents = []

# sftm = torch.nn.Softmax(dim=1)

# for sent in all_sents[:10]:
#   features = tokenizer([sent, 'In the previous sentence, the action is scheduled for future.'], 
#                        [sent, 'In the previous sentence, the action is scheduled for future.'], 
#                       padding=True, truncation=True, return_tensors="pt")
  
#   model.eval()
#   with torch.no_grad():
#       scores = model(**features).logits
#       print(scores)
#       scores = sftm(scores)

#       label_mapping = ['contradiction', 'entailment', 'neutral']

#       new_sents.append((label_mapping[scores.argmax(dim=1)], scores.max(dim=1), sent))



# pose sequence as a NLI premise and label as a hypothesis
from transformers import AutoModelForSequenceClassification, AutoTokenizer
device = "cuda:0" if torch.cuda.is_available() else "cpu"

nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli').to(device)
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

premise = "I am going to USA"
label = "travel"
hypothesis = f'This example is {label}.'

# run through model pre-trained on MNLI
x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                     truncation_strategy='only_first')
logits = nli_model(x.to(device))[0]

# we throw away "neutral" (dim 1) and take the probability of
# "entailment" (2) as the probability of the label being true 
entail_contradiction_logits = logits[:,[0,2]]
probs = entail_contradiction_logits.softmax(dim=1)
prob_label_is_true = probs[:,1]

premise = "I am going to USA"
label = "travel"
hypothesis = f'This example is {label}.'

# run through model pre-trained on MNLI
x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                     truncation_strategy='only_first')
logits = nli_model(x.to(device))[0]

# we throw away "neutral" (dim 1) and take the probability of
# "entailment" (2) as the probability of the label being true 
entail_contradiction_logits = logits[:,[0,2]]
probs = entail_contradiction_logits.softmax(dim=1)
prob_label_is_true = probs[:,1]
print(prob_label_is_true)

premise = "I am going to USA"
hypothesis = 'The previous sentence conatains a future task'

curr_sents = [
    "We'll talk soon.",
    "Like, are you guys having meetings next week, and then we can touch base then.",
    "I will send the invite then.",
    "How are you doing?",
    "Oh, it's still really useful.",
    "What was the address again?",
    "But his email then you're were like, let's do it next week. ",
    "I think it's said it's two weak in December.",
    " Again who said you were sick this morning? ",
    "You gotta find some time next week. ",
    "Although next week, you gotta be careful too. ",
    "Placing see again."
]

for premise in curr_sents:
  # run through model pre-trained on MNLI
  x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                      truncation_strategy='only_first')
  logits = nli_model(x.to(device))[0]

  # we throw away "neutral" (dim 1) and take the probability of
  # "entailment" (2) as the probability of the label being true 
  entail_contradiction_logits = logits[:,[0,2]]
  probs = entail_contradiction_logits.softmax(dim=1)
  prob_label_is_true = probs[:,1].detach().item()
  print(premise, prob_label_is_true)



premise = "I am going to USA"
hypothesis = 'The previous sentence conatains a future task'

new_included_sents = []

for curr in included_sents[:100]:
  _, _, premise = curr
  # run through model pre-trained on MNLI
  x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                      truncation_strategy='only_first')
  logits = nli_model(x.to(device))[0]

  # we throw away "neutral" (dim 1) and take the probability of
  # "entailment" (2) as the probability of the label being true 
  entail_contradiction_logits = logits[:,[0,2]]
  probs = entail_contradiction_logits.softmax(dim=1)
  prob_label_is_true = probs[:,1].detach().item()
  # print(premise, prob_label_is_true)
  new_included_sents.append((prob_label_is_true, curr[2], curr[0], curr[1]))

sorted(new_included_sents, reverse=True)



premise = "I am going to USA"
hypothesis = 'The previous sentence conatains a future task'

new_included_sents = []

for curr in tqdm(included_sents):
  _, _, premise = curr
  # run through model pre-trained on MNLI
  x = tokenizer.encode(premise, hypothesis, return_tensors='pt',
                      truncation_strategy='only_first')
  logits = nli_model(x.to(device))[0]

  # we throw away "neutral" (dim 1) and take the probability of
  # "entailment" (2) as the probability of the label being true 
  entail_contradiction_logits = logits[:,[0,2]]
  probs = entail_contradiction_logits.softmax(dim=1)
  prob_label_is_true = probs[:,1].detach().item()
  # print(premise, prob_label_is_true)
  new_included_sents.append((prob_label_is_true, curr[2], curr[0], curr[1]))



import csv
with open('srlTimeV3.csv','w') as out:
    csv_out=csv.writer(out)
    csv_out.writerow(['label', 'probab', 'sentence', 'time', 'verbs'])
    for row in sorted(new_included_sents, reverse=True):
        csv_out.writerow((1, row[0], row[1], row[2], row[3]))



import pandas as pd
df = pd.read_csv('srlTimeV3_labelled.csv')

df

df[(df.index < 200) & (df['label'] == 1)]

df[(df.index < 200) & (df['label'] == 0)]

pos = df[(df.index < 200) & (df['label'] == 1)]['sentence'].tolist()
neg = df[(df.index < 200) & (df['label'] == 1)]['sentence'].tolist() + df[(df.index > 300)].sample(200)['sentence'].tolist()

pos



from transformers import AutoTokenizer
modelname = 'bert-large-uncased'

tokenizer = AutoTokenizer.from_pretrained(modelname)

def process_data(text, labelgiven=0):
    text = ' '.join(text.split())

    encodings = tokenizer(text, padding="max_length", truncation=True, max_length=128)

    encodings['label'] = labelgiven
    encodings['text'] = text

    return encodings

# !pip install datasets

processed_data = []
for stat in neg:
  processed_data.append(process_data(stat, 0))
for ques in pos:
  processed_data.append(process_data(ques, 1))

from sklearn.model_selection import train_test_split
new_df = pd.DataFrame(processed_data)
train_df, valid_df = train_test_split(new_df, test_size=0.2, random_state=2022)

import pyarrow as pa
from datasets import Dataset

train_hg = Dataset(pa.Table.from_pandas(train_df))
valid_hg = Dataset(pa.Table.from_pandas(valid_df))

train_hg['input_ids']



from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# train_hg['text']

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    modelname,
    num_labels=2
)

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="./result", evaluation_strategy="epoch", num_train_epochs=5, 
                                  warmup_steps=(train_df.shape[0] * 5 // 80), learning_rate=0.00001)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_hg,
    eval_dataset=valid_hg,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()
print(trainer.evaluate())
model.save_pretrained('./model_future_work/')

print(trainer.evaluate())

# ## Load the model

# In[14]:


from transformers import AutoModelForSequenceClassification

new_model = AutoModelForSequenceClassification.from_pretrained('./model/').to(trainer.model.device)


# In[15]:


from transformers import AutoTokenizer

new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')


# ## Get predictions

# In[16]:


import torch
import numpy as np

def get_prediction(text):
    encoding = new_tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

    outputs = new_model(**encoding)

    logits = outputs.logits

    sigmoid = torch.nn.Softmax(dim=-1)
    probs = sigmoid(logits.squeeze().cpu())
    probs = probs.detach().numpy()
    label = np.argmax(probs, axis=-1)
    
    if label == 1:
        return ('question', probs[1])
    else:
        return ('statement', probs[0])


# In[17]:

sents = [
    'what did you have for lunch?',
    'this is urgent, when can you deliver',
    'can you do this in a month',
    'this is good work',
    'can you pull up the numbers for me',
    'why is the delay',
    'how is the baby doing',
    'what are the plans for the weekend',
    'what are the plans for the birthday',
    'I can hear you saying that'
]

for sent in sents:
  print(sent, get_prediction(sent))





