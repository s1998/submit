Host 2: position it aims to help sales teams become more efficient basically and and, you know, sales to coach teams and swollen and were forth and extracting insights from their calls and all and truthful forth. This is the I can I can you introduce cell.

Host 1: Hey. Yeah. I'm engineer simpler and working on sales out in the money. I'm in London in the Uk So I a lot my family they actually start and I never very and I I think you not see needs could been a low less than zero.

Client 3: Yep. Question Meet you. I'll just quickly go since I was speaking to you both Sam gold, senior sales engineer here they're helping with the technical stuff. Of Deepgram I'll let Charlene and Colombo.

Client 4: Good morning, S Ba Coo o for Deepgram. Part of the friends and family of the team flow group. So you guys for having.

Host 2: Great. Totally. Alright. Let me catch you up on the issues that we're having here. So can you see my screen here?

Client 4: Yes.

Host 2: So this is Cell Ai. So the the the idea again, is pretty straightforward we i can see here my meeting recordings. And let me take a good example here, I believe if this this is going to be a final one. And so I can click on any meeting. I can look at the recording This depends how many works workspaces. This is a transcript generated by Deepgram And here, I can I can I can see summary that basically, we feed this transcript to W three. We generate the summary from of the meeting. And then, you know, the user can actually duck to an ai about conversation like, hey, like, what is this conversation about? Like, what was the outcome installing in forth? What we are discovering is that the the prints, correct quality. We're having issues with the transcript quality. Especially, I I I suspect that other that's is actually due to my accent. I think I think it's struggling quite a bit with accents. And I'm looking for like log and pulled in that because this is actually the the best. Mh. Perhaps these Yeah. I think even even with American accents is very often as struggling, So And the problem is that again, we we are fitting that to Ai in instead of like garbage and get a garbage out. And so as a result of this, you know, the the summaries is bad, like, the the the Ai chat is bad and so on and so forth. So, yeah, just went to see if you had any suggestions on how to get better of results. Yeah.

Client 3: So you know, first of all, what we always wanna do is is just take a scientific approach if you can provide the audio if you can combine provide the corresponding transcripts. Information is helpful to us to understand exactly what's going wrong. Right? When you say something like you know, they're they're not great on my accent. Right? What's that mean is it not picking of any words? Is it picking up some words? Is it missing words. Right? So so identifying exactly what's going wrong is going to be the first step that we're gonna wanna take. The second step we're gonna wanna take is if this is something that we can pick up very quickly through model training. Basically, what we would do is we would take some representative audio what's in your in, which I understand is probably not representative but everyone using sales dot ai. But we just get data from it. And we normalize against just a general set of audio meaning all audio, all human voices. But we train on some of yours. We can improve on your specific accent and then give you a model that's more generally so you don't get negative you know, accuracy against our base model. But we also improved greatly the accuracy specifically on your ex or others like yours. Does that make sense?

Host 2: Yeah. The problem is that obviously, we'll not be able to do that my reasonable customer and and we'd rather avoid having to go through training room. Even if we could, we'd would rather avoid throwing a away to first two meetings because like that's like the the impression of the meaning of the of the of the product is is fooled. This is another example that we made. So just To characterize very clearly Easy issue again. So... yeah. It basically... Oh. Guys... Yeah. I spent like semester in france long ago in Mobile so... But I wanna go to everything ma'am. Okay. What what were are you doing called? Physics. I was a physics researchers. So it was my there. Just semester. Oh, wow. Nice. Right. So here, it it's it's like message reading the Nice It's... It he also has an accent, which I think is the other issue like he's got like a russian accent, but it's it's basically, I think more than half the world all incorrect. I don't know last. Alright. Just... Yeah. So, yeah, That that's the issue we're having here. So is there any solution here above like, we're retraining the mobile on like can you license?

Client 3: So so there's two things here. Right? Because what you're pointing out is that the words are distributed incorrectly to the speaker. Which i deem is different than the words being incorrect. Right?

Client 3: So which were discussing Sorry go ahead.

Host 2: That does both.

Host 1: I think the bugs are incorrect. Just well, like if you looked Soogrim does deal last year, on the screen. I think i said I moved here last year, so it's quite different what it's coming back with.

Host 2: Described. My is very.

Host 2: That's here. It used to be in the bay area nine who does heal last year.

Client 3: Okay. Yeah. So I think you know, the first thing is if these are multi channel recordings, which I don't know what they are, you can actually separate based on channel and then you're essentially going to have this as a perfect separation. Because everyone every everywhere that, like, flow says will be a transcript dedicated to flow and everywhere in that Sergei size will be a transcript dedicated to Sergei. So that's another approach if you do have channel separated audio. Otherwise, dia is doing its best to essentially assign speakers and that's maybe a little bit different than explicitly just the words being wrong. That's more of a dia and and then I'd kind of ask you more questions than that route. In terms of the accuracy, I know it's not ideal for me to suggest a model training and I wouldn't expect that we would want to retrain for every single customer. Maybe some initial training just based on your meetings, the subject context of your meetings. We're not just training on your accents. We're also training on the audio context and the specific things being spoken here. So the conversations that you're going to be having are going to be different in the conversations that know call center might be having or equivalent. Right? Because they're more sales focused. The other thing is we just have to understand which parameters you're using though I you're using the model. So that shouldn't be you know, a blocker in terms of you getting our best accuracy. Are using, you know, enhanced phone call or enhanced general? Can you share maybe more about that?

Host 2: I'm believe using enhanced general d know ian? Yes?

Host 1: Sorry. It was muted. Yeah. I think we're using it in enhanced were using some of something think it's in general. And I think we're using some enhanced dia sterilization as well. Attention Okay.

Client 3: If you can just confirm that, maybe try to enhanced phone calls folks and these are more kind of like phone call meetings. That's that's under there happening you can take might be a better model.

Host 2: If separately.

Host 1: Agent They come from video conferencing like Zoom google meet.

Host 1: But i I think those are the main two. So with that with phone call apply or In like then have literally is the phone call thing, but don't think it's more yellow will band audio.

Client 3: Not specific explicitly phone call. It's more just aligned to low band. So you know, I don't know how you're pulling the recordings, but if they are like, hey k Eight kilohertz eight thousand kilohertz. Then, you know, that might be something where the phone come on might be more performant.

Host 2: Doesn't sound like according to me.

Client 3: Does it? Yeah.

Client 3: From from this. This one does.

Host 2: So... Okay. So check, please recall with the recordings or h k. I think should also check with. we can have channel based recordings like multi channel recordings.

Client 3: Yeah.

Client 3: Those are two good shops to start with. And then I'd also just ask, you know, again, I wouldn't necessarily recommend you do model for every single customer though you might want to do strategic ones especially if you have, like, a large customer worth a large portion of revenue or whatever. Models tend to perform very well. Because you're training on their specific context verbiage, terminology spellings, etcetera. Sound environment acoustic environment. What I might suggest though is just a small you and model train. Just so you can see for yourself based on your audio, and your representative conversations. We can whip one up that's, you know, not broader than maybe this more specific use case, if it seems like this is more interview type conversations you know, we we could train on that and and you would probably see a pretty significant improvement based on being more inclusive of this specific the accents and the specific terminology being discussed in these conversations.

Host 1: Training with we need a transcript that's been corrected. If there any missing words and anything done that.

Client 3: Yeah. Great question. No. You would just provide us the audio. So it would just be representative audio. So, you know, this is an example. You know, audio that you'd be running the meeting stock we're on. You would pull this audio file or video file just the original one that we wouldn't want to compress it or change in any way because, essentially, what we're training on is also we're going to be improving on. So I would say, you know, ten and least hours it's a representative audio, if you can provide twenty, that'd be much much better. We will do the labeling and output the model to you that is, let's say, a model equals steam flow. And then you wouldn't even have to worry about, you know, phone call or general because we basically validate that on our end, make sure we identify which ones more performing. And then output a model to you. That is just model equals flow your equals hands. That kind of takes care of all of that together and is better on the a big accents and terminology that you discuss.

Host 2: But, I I don't assume that this would transfer is gonna to performance games would transfer to other customers with it.

Client 3: So it it does depend. Right? If your other customers having some more conversations to this or some accents, then it very much would transfer because it's training on not just, you know, your accent, but the features of your accent, the acoustic environment of your accent the terminology of your accent.

Client 3: Don't think I don't think they are having similar musicians.

Host 2: Neither on the x on the topic and and I I think they're also having, like, issues with use the transcriptions as well.

Host 2: I mean, you wanna send us representative audio from them, we could train a model specifically for the customer as well.

Client 3: Right? And and again, I'm not saying the trip to for every single customer. But getting a baseline of some amount of hours of trouble audio and don't just buy as towards the trouble audio also give us maybe we also do quite well out because we wanna just get a baseline of here's the type of things that are discussed. Here's the type of accents involved here the type of room environment or acoustic environment, Right? Is it two speakers is the four speaker six speakers. All of that data goes in and we output a model for you. That is, you know, more bias towards getting that information directly.

Host 2: Have the dumb question. Jot from Loom is in a strange employee. He was telling me that they were experimenting quite a bit right now on based transcription and they were having extraordinary results based on that. Like, this is like, blue, like, all of their benchmarks. Is that something that Deepgram is is currently looking at?

Client 3: Yeah. We've looked at whisper Soogrim our Ceo as a big blog post about it. Broadly speaking, there's a few different faults with Whisper. And, you know, I I could get into it if it's relevant to you here, but We looked at it and based on our findings we found that we format on the number of benchmarks. The other thing that Wi doesn't have going for it? Is real time transcription, which it doesn't currently do. Whisper does not have time stamps. Might make some of your transcript logic a little bit more difficult. And Whisper is also much slower. So you we noticed in running Deepgram it outputs the results very quickly, whereas Whisper takes a bit of time longer. And all that's based on the approach. So we've kind patented our approach. Has been proven to be a lot more computer efficient and has a lot of other benefits again, to the approach that we've taken.

Host 2: How much slower is we period because In case, you know, I I I actually think holy is a lot more important in than speed. For now, at least yeah.

Client 3: Damn i'm absolutely Here is the blog post chat. It has information about the speed about the accuracy, about the different models, we sent to all five models the tiny and the base to small the medium in the orange. And the speeds as well. Relative each model.

Client 5: One one thing to add is that our research team actually has work on getting whisper into our system and also supporting the features that we have. So like, the if if you were to download Whisper off the shelf right now, you wouldn't get timestamps stamps and and features that work with Deepgram essentially. We are working to make that available to customers and that should be ready and January, maybe even as soon as next week. So some datasets we actually have seen that whisper out performs our models like Sam on others and some of internal benchmarks that we've done. We've outperformed them. So it kinda depends on the dataset. But we will have that option available and, like, fully featured for our customers. There are some implications around the compute side of it, sam was mentioning, like, an hour you, a thirty minute phone call for Deepgram would take, like, fifteen, twenty seconds to return to you for for what spur depending on the model that you're running, it could take anywhere between, like, thirty minutes in a few hours. But if that's not a problem, you're getting better accuracy and you want, like, the time stamps and other features like that. Like, you know, dia or multi channel to, like, just work as opposed to getting it off the shelf yourself. That's that's something that we could discuss probably relatively soon have you be one of our early testers of that?

Host 2: Yeah. I'd love... I have to at least like run box on Luis against like this this kind of transcript, for example, and and see the kind of results that we're getting. I'm wondering there would be workaround workarounds to the timestamp issues, for example. Like, maybe if you're running to like your own model and then you use the time for you from your model, but the transcripts from luis pure, this is kind of stuff Yeah.

Client 5: So the the work... Like, the the things that we've been doing on our end is getting our like, feature set to to work with Whisper. So, like, what you get with the Deepgram enhanced model, you would also get with Whisper in terms of, like, the time stamps and other features that are supported. So that's been kind of the the lift of what the the research team has been doing, and then we gotta put it in production.

Host 2: And and when do you think they'll be available for testing?

Client 5: As soon as next week as late as like, end of January, So pretty I I love to say up for the middle of that definitely.

Client 5: Okay. Sounds good. But we'll definitely let you know, like, right when something's released, and and in production will we'll let you know.

Host 2: Sweet. Also i'll Okay. So I'll put up in the week belt whisper and the the the bit out of that model. Great. Anything else we So on our side, I think Yeah. We're going to send you a bunch of of our audio, I would love to get like, that custom model we mentioned. I think we also need to talk without partner because we don't do the recordings all sales. We we go through a partner that's called a recall. You might be to have filled him. And I I am another impression that i might be doing some wrong with the recording time I mean the person to make they might not do like a general recording and it might do, like an eight k kind of recording. So like, when you we need to check with them what's going on here. Anything else we can do here.

Client 3: Offhand. Know I'd always recommend Just testing it out testing some of the other parameters.

Client 3: You've mentioned you've only tested to general phone calls a good shout as well. Recall, you know, that's probably a good place to start to figure out if the recordings are changing in any way from, you know, the initial to the recording you know, any type of compression you're doing is helpful as a reference point as well. And the model training is really going to be the one that's going to be the fat this you know, improvement that you're going to see. Because again, if we're training on the specific representative audio, we're going to improve significant on that audio. And middle yield up you know, pretty substantial improvement.

Host 2: Great. Awesome. Okay? Okay. So if me follow the email then with do list of these action items. Ian will be able to to run point on them. So again, your representative audio and, you know, testing a few more parameters like playing around with with this, basically. And we can take it from there.

Client 3: Yes. Sounds great. Just quickly while I have you both should we book time for next week to check in? Or I'm ian are you planning on sending the audio I mean, should I just check with ian in a week? Like? Like, what's the best plan there?

Host 2: Yeah. How much could do that by email? What what do you think?

Host 1: Yeah. Should we just do that on the existing thing email thread and then we can because we'll go ask some questions partner or about some of the things and and see how much audio we've got so far to give you training in the something.

Client 3: works. Yeah. So whenever you get that, send it over, I'll pull it down and send it to our research team. To make that model. And then we deploy that model especially for you. So, like, only your project. So one of the other things you might wanna look into giving me? Is the email that you're using the project Id that you're using Typically, we also segment access to models by the project Id, and you can invite members into the project. So that's another thing that you might wanna think about is if you have, like, an overarching team project, make sure everyone that would be testing the model is in that project.

Host 1: Okay. So Yeah. When you say everyone who would be testing that model, Do you mean because we have a project Id, and we use the Api api to send the audio to that. So you just mean that we're sending everything to that right. So that's to the same project.

Client 3: To that project, yeah. You can make multiple You can make multiple email accounts associated one project But we're going to give access to that model that we eventually you project Id.

Client 3: So just make sure that yeah. Anyone who's testing it is part of that project I. Yep. Cool Alright?

Host 2: Great. Okay. I'm following up then by email and we can take it from there.

Client 3: Sounds good. Meeting both. Thank you for their time.

Host 2: Thank you as well. So yep.

Host 1: See Thank